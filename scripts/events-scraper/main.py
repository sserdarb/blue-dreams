"""
Bodrum Events Scraper
=====================
crawl4ai + Gemini 1.5 Flash ile Bodrum etkinliklerini Ã§eken async script.
Ã‡Ä±ktÄ±: bodrum_events.json

Kurulum:
    pip install crawl4ai google-generativeai pydantic python-dotenv
    playwright install
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from typing import Optional

from dotenv import load_dotenv
from pydantic import BaseModel, Field

# .env dosyasÄ±ndan API anahtarlarÄ±nÄ± oku
load_dotenv()

# ============ Pydantic Modeller ============

class BodrumEvent(BaseModel):
    """Tek bir etkinlik iÃ§in veri modeli."""
    event_name: str = Field(..., description="EtkinliÄŸin adÄ±")
    event_date: str = Field(..., description="ISO 8601 formatÄ±nda tarih (YYYY-MM-DD)")
    event_time: str = Field(default="", description="Saat (HH:MM formatÄ±nda)")
    location: str = Field(default="Bodrum", description="Mekan adÄ±")
    ticket_url: str = Field(default="", description="Bilet/detay linki")
    description: str = Field(default="", description="KÄ±sa aÃ§Ä±klama")
    category: str = Field(default="", description="Etkinlik kategorisi (konser, tiyatro, festival vb.)")
    image_url: str = Field(default="", description="Etkinlik gÃ¶rseli URL'si")


class EventsResult(BaseModel):
    """TÃ¼m etkinliklerin sonuÃ§ modeli."""
    events: list[BodrumEvent] = []
    scraped_at: str = Field(default_factory=lambda: datetime.now().isoformat())
    source_url: str = ""
    total_count: int = 0


# ============ Scraping FonksiyonlarÄ± ============

# Hedef URL'ler
TARGET_URLS = [
    "https://www.biletix.com/search/TURKIYE/tr?searchq=bodrum",
    "https://www.passo.com.tr/tr/sehir/bodrum",
]


async def scrape_page(url: str) -> Optional[str]:
    """
    crawl4ai ile sayfayÄ± Ã§eker ve Markdown formatÄ±nda dÃ¶ndÃ¼rÃ¼r.
    TarayÄ±cÄ± aÃ§Ä±p JavaScript render yapabilir.
    """
    try:
        from crawl4ai import AsyncWebCrawler
        from crawl4ai import CrawlerRunConfig

        config = CrawlerRunConfig(
            word_count_threshold=50,  # KÄ±sa paragraflarÄ± filtrele
            wait_until="domcontentloaded",
            page_timeout=30000,
        )

        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=config)

            if result.success and result.markdown:
                print(f"âœ… Sayfa baÅŸarÄ±yla Ã§ekildi: {url}")
                print(f"   Markdown uzunluÄŸu: {len(result.markdown)} karakter")
                # Token limitini aÅŸmamak iÃ§in ilk 15000 karakter
                return result.markdown[:15000]
            else:
                print(f"âš ï¸  Sayfa Ã§ekilemedi: {url}")
                return None

    except ImportError:
        print("âŒ crawl4ai kÃ¼tÃ¼phanesi bulunamadÄ±. pip install crawl4ai")
        return None
    except Exception as e:
        print(f"âŒ Scraping hatasÄ± ({url}): {e}")
        return None


async def extract_events_with_gemini(markdown_content: str, source_url: str) -> list[BodrumEvent]:
    """
    Markdown iÃ§eriÄŸini Gemini 1.5 Flash modeline gÃ¶nderip
    yapÄ±landÄ±rÄ±lmÄ±ÅŸ etkinlik verisi Ã§Ä±karÄ±r.
    """
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GEMINI_API_KEY environment variable bulunamadÄ±!")
        print("   .env dosyasÄ±na GEMINI_API_KEY=your_key ekleyin.")
        return []

    try:
        import google.generativeai as genai

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash")

        current_year = datetime.now().year

        prompt = f"""
Sen bir etkinlik veri Ã§Ä±karma uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki web sayfasÄ± iÃ§eriÄŸinden 
Bodrum'da gerÃ§ekleÅŸecek etkinlikleri (konserler, festivaller, tiyatro, sergiler vb.) Ã§Ä±kar.

KURALLAR:
1. Navigasyon, footer, reklam gibi gereksiz metinleri yok say.
2. Sadece etkinlik listesine odaklan.
3. Tarihler ISO 8601 formatÄ±nda (YYYY-MM-DD) olmalÄ±. YÄ±l yoksa {current_year} varsay.
4. Saatler HH:MM formatÄ±nda olmalÄ±.
5. Etkinlik linki varsa ticket_url alanÄ±na ekle, yoksa boÅŸ bÄ±rak.
6. Kategoriyi belirle: konser, tiyatro, festival, sergi, spor, parti, workshop vb.
7. Her etkinlik iÃ§in kÄ±sa bir aÃ§Ä±klama yaz.

Ã‡IKTI FORMATI (JSON dizisi):
[
    {{
        "event_name": "Etkinlik AdÄ±",
        "event_date": "2025-07-15",
        "event_time": "21:00",
        "location": "Mekan AdÄ±, Bodrum",
        "ticket_url": "https://...",
        "description": "KÄ±sa aÃ§Ä±klama",
        "category": "konser",
        "image_url": ""
    }}
]

EÄŸer hiÃ§ etkinlik bulunamazsa boÅŸ dizi [] dÃ¶ndÃ¼r.
YanÄ±tÄ±nda SADECE JSON dizisini ver, baÅŸka bir ÅŸey yazma.

---
SAYFA Ä°Ã‡ERÄ°ÄÄ°:
{markdown_content}
"""

        response = await asyncio.to_thread(
            model.generate_content, prompt
        )

        # JSON'u parse et
        text = response.text.strip()
        # Markdown code block varsa temizle
        if text.startswith("```"):
            text = text.split("\n", 1)[1]
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()

        events_data = json.loads(text)

        events = []
        for item in events_data:
            try:
                event = BodrumEvent(**item)
                events.append(event)
            except Exception as e:
                print(f"âš ï¸  Etkinlik parse hatasÄ±: {e}")
                continue

        print(f"âœ… {len(events)} etkinlik Ã§Ä±karÄ±ldÄ± ({source_url})")
        return events

    except ImportError:
        print("âŒ google-generativeai kÃ¼tÃ¼phanesi bulunamadÄ±. pip install google-generativeai")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSON parse hatasÄ±: {e}")
        return []
    except Exception as e:
        print(f"âŒ Gemini API hatasÄ±: {e}")
        return []


async def scrape_all_events() -> EventsResult:
    """TÃ¼m hedef URL'leri tarar ve etkinlikleri toplar."""
    all_events: list[BodrumEvent] = []

    for url in TARGET_URLS:
        print(f"\nğŸ” Taraniyor: {url}")
        markdown = await scrape_page(url)

        if markdown:
            events = await extract_events_with_gemini(markdown, url)
            all_events.extend(events)

    # SonuÃ§larÄ± oluÅŸtur
    result = EventsResult(
        events=all_events,
        source_url=", ".join(TARGET_URLS),
        total_count=len(all_events),
    )

    return result


def save_results(result: EventsResult, output_path: str = "bodrum_events.json"):
    """SonuÃ§larÄ± JSON dosyasÄ±na kaydet."""
    output = result.model_dump()

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ SonuÃ§lar kaydedildi: {output_path}")
    print(f"   Toplam etkinlik sayÄ±sÄ±: {result.total_count}")


# ============ Ana Program ============

async def main():
    """Ana asenkron fonksiyon."""
    print("=" * 60)
    print("ğŸ­ Bodrum Etkinlikleri Scraper")
    print(f"ğŸ“… Ã‡alÄ±ÅŸtÄ±rma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    result = await scrape_all_events()

    if result.total_count > 0:
        save_results(result)
    else:
        print("\nâš ï¸  HiÃ§ etkinlik bulunamadÄ±.")
        # BoÅŸ bile olsa dosyayÄ± oluÅŸtur
        save_results(result)

    print("\nâœ¨ TamamlandÄ±!")
    return result


if __name__ == "__main__":
    asyncio.run(main())
